{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center> Emotion Recognition using BiLSTM\n    ","metadata":{}},{"cell_type":"markdown","source":"Aims to classify the 6 different emotions (sadness, anger, love, surprise, fear, joy) using BiLSTM","metadata":{}},{"cell_type":"markdown","source":"# Importing the Libraries","metadata":{}},{"cell_type":"code","source":"!pip install nlp\n!pip install datasets\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom wordcloud import WordCloud\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport nlp\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Dropout\nfrom keras.layers import LSTM\nfrom keras.models import Sequential\nfrom keras.layers import Embedding\nfrom keras.layers import Flatten\nfrom keras.layers import Bidirectional\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import GlobalAvgPool1D\nimport random\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-09-13T05:23:01.59581Z","iopub.execute_input":"2021-09-13T05:23:01.596452Z","iopub.status.idle":"2021-09-13T05:23:34.384435Z","shell.execute_reply.started":"2021-09-13T05:23:01.596324Z","shell.execute_reply":"2021-09-13T05:23:34.382951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing the Dataset\n\n**Detailed dataset info: https://huggingface.co/datasets/emotion**","metadata":{}},{"cell_type":"code","source":"# Importing the dataset\ndata = nlp.load_dataset('emotion')\n\n# Converting the train, validation and test datasets into DataFrame format\ntrain = pd.DataFrame(data['train'])\nvalidation = pd.DataFrame(data['validation'])\ntest = pd.DataFrame(data['test'])","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-09-13T05:23:34.386584Z","iopub.execute_input":"2021-09-13T05:23:34.38705Z","iopub.status.idle":"2021-09-13T05:23:48.393716Z","shell.execute_reply.started":"2021-09-13T05:23:34.387008Z","shell.execute_reply":"2021-09-13T05:23:48.392574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train dataset\ntrain.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T05:23:48.396683Z","iopub.execute_input":"2021-09-13T05:23:48.397309Z","iopub.status.idle":"2021-09-13T05:23:48.425574Z","shell.execute_reply.started":"2021-09-13T05:23:48.397262Z","shell.execute_reply":"2021-09-13T05:23:48.424334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check the unique labels of the dataset\ntrain['label'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T05:23:48.42756Z","iopub.execute_input":"2021-09-13T05:23:48.428069Z","iopub.status.idle":"2021-09-13T05:23:48.478574Z","shell.execute_reply.started":"2021-09-13T05:23:48.428025Z","shell.execute_reply":"2021-09-13T05:23:48.477237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Distribution of the Length of the Texts","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:47:37.333036Z","iopub.execute_input":"2021-08-02T12:47:37.333432Z","iopub.status.idle":"2021-08-02T12:47:37.337751Z","shell.execute_reply.started":"2021-08-02T12:47:37.333397Z","shell.execute_reply":"2021-08-02T12:47:37.336527Z"}}},{"cell_type":"code","source":"train['length_of_text'] = [len(i.split(' ')) for i in train['text']]\n\nfig = px.histogram(train['length_of_text'], marginal='box',\n                   labels={\"value\": \"Length of the Text\"})\n\nfig.update_traces(marker=dict(line=dict(color='#000000', width=2)))\nfig.update_layout(title_text='Distribution of the Length of the Texts',\n                  title_x=0.5, title_font=dict(size=22))\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-13T05:23:48.48042Z","iopub.execute_input":"2021-09-13T05:23:48.48099Z","iopub.status.idle":"2021-09-13T05:23:49.67916Z","shell.execute_reply.started":"2021-09-13T05:23:48.480946Z","shell.execute_reply":"2021-09-13T05:23:49.677966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The length of the data points is distributed between 4 to 46. The outliers start from 48 words.**","metadata":{}},{"cell_type":"markdown","source":"# Distribution of the Length of the Texts by Emotions\n","metadata":{}},{"cell_type":"code","source":"fig = px.histogram(train['length_of_text'], marginal='box',\n                   labels={\"value\": \"Length of the Text\"},\n                   color=train['label'])\nfig.update_traces(marker=dict(line=dict(color='#000000', width=2)))\nfig.update_layout(title_text='Distribution of the Length of the Texts by Emotions',\n                  title_x=0.5, title_font=dict(size=22))\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-13T05:23:49.680483Z","iopub.execute_input":"2021-09-13T05:23:49.680917Z","iopub.status.idle":"2021-09-13T05:23:49.911902Z","shell.execute_reply.started":"2021-09-13T05:23:49.680861Z","shell.execute_reply":"2021-09-13T05:23:49.910456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Distribution of the Labels\n","metadata":{}},{"cell_type":"code","source":"fig = px.histogram(train, x='label', color='label')\nfig.update_traces(marker=dict(line=dict(color='#000000', width=2)))\nfig.update_layout(title_text='Distribution of the Labels',\n                  title_x=0.5, title_font=dict(size=22))\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-13T05:23:49.913846Z","iopub.execute_input":"2021-09-13T05:23:49.914319Z","iopub.status.idle":"2021-09-13T05:23:50.203999Z","shell.execute_reply.started":"2021-09-13T05:23:49.914278Z","shell.execute_reply":"2021-09-13T05:23:50.202807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Frequency of the Words in the Train Dataset\n","metadata":{}},{"cell_type":"code","source":"FreqOfWords = train['text'].str.split(expand=True).stack().value_counts()\nFreqOfWords_top200 = FreqOfWords[:200]\n\nfig = px.treemap(FreqOfWords_top200, path=[FreqOfWords_top200.index], values=0)\nfig.update_layout(title_text='Frequency of the Words in the Train Dataset',\n                  title_x=0.5, title_font=dict(size=22)\n                  )\nfig.update_traces(textinfo=\"label+value\")\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-13T05:23:50.207598Z","iopub.execute_input":"2021-09-13T05:23:50.208088Z","iopub.status.idle":"2021-09-13T05:23:50.85196Z","shell.execute_reply.started":"2021-09-13T05:23:50.208048Z","shell.execute_reply":"2021-09-13T05:23:50.850596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**According to graph above, the most frequent words include stopwords such as \"i\", \"and\", \"to\", etc. For the further steps, I will remove them.**","metadata":{}},{"cell_type":"markdown","source":"# Tokenizing with NLTK\n","metadata":{}},{"cell_type":"code","source":"def tokenization(inputs):\n    return word_tokenize(inputs) #REFERENCE[1]\n\n\ntrain['text_tokenized'] = train['text'].apply(tokenization)\nvalidation['text_tokenized'] = validation['text'].apply(tokenization)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T05:23:50.856961Z","iopub.execute_input":"2021-09-13T05:23:50.860798Z","iopub.status.idle":"2021-09-13T05:23:54.771456Z","shell.execute_reply.started":"2021-09-13T05:23:50.860695Z","shell.execute_reply":"2021-09-13T05:23:54.770363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T05:23:54.773059Z","iopub.execute_input":"2021-09-13T05:23:54.773477Z","iopub.status.idle":"2021-09-13T05:23:54.794623Z","shell.execute_reply.started":"2021-09-13T05:23:54.773436Z","shell.execute_reply":"2021-09-13T05:23:54.792805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**By using tokenization, I split each data point into words. Tokenization is one of the key steps for NLP applications.**","metadata":{}},{"cell_type":"markdown","source":"# Stopwords Removal\n","metadata":{}},{"cell_type":"markdown","source":"**As we have seen from the Frequency of the Words in the Train Dataset visualization, the most frequent words were the English stopwords such as \"i\", \"you\", \"their\", \"to\", etc. In this step, we will remove these words from the entire dataset by using the NLTK library.**","metadata":{}},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\n\ndef stopwords_remove(inputs):\n    return [item for item in inputs if item not in stop_words]\n\ntrain['text_stop'] = train['text_tokenized'].apply(stopwords_remove)\nvalidation['text_stop'] = validation['text_tokenized'].apply(stopwords_remove)\n\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T05:23:54.79693Z","iopub.execute_input":"2021-09-13T05:23:54.797529Z","iopub.status.idle":"2021-09-13T05:23:55.140168Z","shell.execute_reply.started":"2021-09-13T05:23:54.797485Z","shell.execute_reply":"2021-09-13T05:23:55.138816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**According to the first 5 rows of the train dataset, it is obvious that we achieved our goal.**","metadata":{}},{"cell_type":"markdown","source":"# Lemmatization","metadata":{}},{"cell_type":"markdown","source":"**Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word [2].** ","metadata":{}},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\n\ndef lemmatization(inputs):\n    return [lemmatizer.lemmatize(word=x, pos='v') for x in inputs]\n\ntrain['text_lemmatized'] = train['text_stop'].apply(lemmatization)\nvalidation['text_lemmatized'] = validation['text_stop'].apply(lemmatization)\n\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T05:23:55.142261Z","iopub.execute_input":"2021-09-13T05:23:55.142764Z","iopub.status.idle":"2021-09-13T05:23:58.143917Z","shell.execute_reply.started":"2021-09-13T05:23:55.14269Z","shell.execute_reply":"2021-09-13T05:23:58.142916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Joining Tokens into Sentences\n","metadata":{}},{"cell_type":"code","source":"train['text_cleaned'] = train['text_lemmatized'].str.join(' ')\nvalidation['text_cleaned'] = validation['text_lemmatized'].str.join(' ')\n\ntrain.head() # Final form of the dataset","metadata":{"execution":{"iopub.status.busy":"2021-09-13T05:23:58.145555Z","iopub.execute_input":"2021-09-13T05:23:58.146027Z","iopub.status.idle":"2021-09-13T05:23:58.190669Z","shell.execute_reply.started":"2021-09-13T05:23:58.145971Z","shell.execute_reply":"2021-09-13T05:23:58.189562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# WordCloud of the Cleaned Dataset\n","metadata":{}},{"cell_type":"code","source":"WordCloud = WordCloud(max_words=100,\n                      random_state=30,\n                      collocations=True).generate(str((train['text_cleaned'])))\n\nplt.figure(figsize=(15, 8))\nplt.imshow(WordCloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-13T05:23:58.192616Z","iopub.execute_input":"2021-09-13T05:23:58.193359Z","iopub.status.idle":"2021-09-13T05:23:58.837121Z","shell.execute_reply.started":"2021-09-13T05:23:58.193315Z","shell.execute_reply":"2021-09-13T05:23:58.835671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizing with Tensorflow\n","metadata":{}},{"cell_type":"code","source":"num_words = 15000\ntokenizer = Tokenizer(num_words=num_words, oov_token='<OOV>')\ntokenizer.fit_on_texts(train['text_cleaned'])\n\nword_index = tokenizer.word_index \n# print(word_index) ","metadata":{"execution":{"iopub.status.busy":"2021-09-13T05:23:58.838786Z","iopub.execute_input":"2021-09-13T05:23:58.839377Z","iopub.status.idle":"2021-09-13T05:23:59.136304Z","shell.execute_reply.started":"2021-09-13T05:23:58.839322Z","shell.execute_reply":"2021-09-13T05:23:59.135213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Tokenized_train = tokenizer.texts_to_sequences(train['text_cleaned'])\nTokenized_val = tokenizer.texts_to_sequences(validation['text_cleaned'])","metadata":{"execution":{"iopub.status.busy":"2021-09-13T05:23:59.137925Z","iopub.execute_input":"2021-09-13T05:23:59.138404Z","iopub.status.idle":"2021-09-13T05:23:59.390948Z","shell.execute_reply.started":"2021-09-13T05:23:59.138362Z","shell.execute_reply":"2021-09-13T05:23:59.389867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Some Tokenziation Examples from the Dataset**","metadata":{}},{"cell_type":"code","source":"print('Non-tokenized Version: ', train['text_cleaned'][0])\nprint('Tokenized Version: ', tokenizer.texts_to_sequences([train['text_cleaned'][0]]))\nprint('--'*50)\nprint('Non-tokenized Version: ', train['text_cleaned'][10])\nprint('Tokenized Version: ', tokenizer.texts_to_sequences([train['text_cleaned'][10]]))\nprint('--'*50)\nprint('Non-tokenized Version: ', train['text'][100])\nprint('Tokenized Version: ', tokenizer.texts_to_sequences([train['text_cleaned'][100]]))","metadata":{"execution":{"iopub.status.busy":"2021-09-13T05:23:59.392612Z","iopub.execute_input":"2021-09-13T05:23:59.393155Z","iopub.status.idle":"2021-09-13T05:23:59.407067Z","shell.execute_reply.started":"2021-09-13T05:23:59.393114Z","shell.execute_reply":"2021-09-13T05:23:59.404971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Padding","metadata":{}},{"cell_type":"code","source":"maxlen = 50\nPadded_train = pad_sequences(Tokenized_train, maxlen=maxlen, padding='pre')\nPadded_val = pad_sequences(Tokenized_val, maxlen=maxlen, padding='pre')\n\nprint('Non-padded Version: ', tokenizer.texts_to_sequences([train['text_cleaned'][0]]))\nprint('Padded Version: ', Padded_train[0])\nprint('--'*50)\nprint('Non-padded Version: ', tokenizer.texts_to_sequences([train['text_cleaned'][10]]))\nprint('Padded Version: ', Padded_train[10])\n","metadata":{"execution":{"iopub.status.busy":"2021-09-13T05:23:59.408908Z","iopub.execute_input":"2021-09-13T05:23:59.409316Z","iopub.status.idle":"2021-09-13T05:23:59.57876Z","shell.execute_reply.started":"2021-09-13T05:23:59.409281Z","shell.execute_reply":"2021-09-13T05:23:59.577593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating the Model\n","metadata":{}},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Embedding(num_words, 300, input_length=maxlen))\nmodel.add(GlobalAvgPool1D())\n\ntf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True, activation='relu'))\nmodel.add(Dropout(0.2))\n\ntf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, activation='relu', return_sequences=True))\nmodel.add(Dropout(0.2))\n\ntf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, activation='relu', return_sequences=False))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(6, activation='softmax'))\n\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T05:23:59.580298Z","iopub.execute_input":"2021-09-13T05:23:59.580682Z","iopub.status.idle":"2021-09-13T05:24:07.275803Z","shell.execute_reply.started":"2021-09-13T05:23:59.580642Z","shell.execute_reply":"2021-09-13T05:24:07.274777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the Model\n","metadata":{}},{"cell_type":"code","source":"# Replacing the string labels with integers\nlabel_ = {\"sadness\": 0, \"joy\": 1, \"love\": 2, \"anger\": 3, \"fear\": 4, \"surprise\": 5}\ntrain['label'] = train['label'].replace(label_)\nvalidation['label'] = validation['label'].replace(label_)\n\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T05:24:07.277244Z","iopub.execute_input":"2021-09-13T05:24:07.277811Z","iopub.status.idle":"2021-09-13T05:24:07.326088Z","shell.execute_reply.started":"2021-09-13T05:24:07.27777Z","shell.execute_reply":"2021-09-13T05:24:07.324872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='auto', patience=5,\n                                                 restore_best_weights=True)\n\nepochs = 100\nhist = model.fit(Padded_train, train['label'], \n                 epochs=epochs,\n                 validation_data=(Padded_val, validation['label']), \n                 callbacks=[early_stopping]\n                )","metadata":{"execution":{"iopub.status.busy":"2021-09-13T05:24:07.327564Z","iopub.execute_input":"2021-09-13T05:24:07.328019Z","iopub.status.idle":"2021-09-13T05:31:02.480846Z","shell.execute_reply.started":"2021-09-13T05:24:07.327977Z","shell.execute_reply":"2021-09-13T05:31:02.479711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train and Validation Loss Graphs\n","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15, 8))\nplt.plot(hist.history['loss'], label='Train Loss')\nplt.plot(hist.history['val_loss'], label='Validation Loss')\nplt.title('Train and Validation Loss Graphs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T05:31:02.482843Z","iopub.execute_input":"2021-09-13T05:31:02.483559Z","iopub.status.idle":"2021-09-13T05:31:02.765704Z","shell.execute_reply.started":"2021-09-13T05:31:02.48352Z","shell.execute_reply":"2021-09-13T05:31:02.764474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing the Test Data\n","metadata":{}},{"cell_type":"code","source":"test['text_tokenized'] = test['text'].apply(tokenization)\ntest['text_stop'] = test['text_tokenized'].apply(stopwords_remove)\ntest['text_lemmatized'] = test['text_stop'].apply(lemmatization)\ntest['text_cleaned'] = test['text_lemmatized'].str.join(' ')\n\nTokenized_test = tokenizer.texts_to_sequences(test['text_cleaned'])\nPadded_test = pad_sequences(Tokenized_test, maxlen=maxlen, padding='pre')\n\ntest['label'] = test['label'].replace(label_)\n\ntest_evaluate = model.evaluate(Padded_test, test['label'])","metadata":{"execution":{"iopub.status.busy":"2021-09-13T05:31:02.770883Z","iopub.execute_input":"2021-09-13T05:31:02.771163Z","iopub.status.idle":"2021-09-13T05:31:03.573776Z","shell.execute_reply.started":"2021-09-13T05:31:02.771135Z","shell.execute_reply":"2021-09-13T05:31:03.572665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Making Predictions in the Test Data\n","metadata":{}},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T05:31:03.576055Z","iopub.execute_input":"2021-09-13T05:31:03.576399Z","iopub.status.idle":"2021-09-13T05:31:03.599767Z","shell.execute_reply.started":"2021-09-13T05:31:03.576371Z","shell.execute_reply":"2021-09-13T05:31:03.59844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_predictions(text_input):\n    text_input = str(text_input)\n    text_input = tokenization(text_input)\n    text_input = stopwords_remove(text_input)\n    text_input = lemmatization(text_input)\n    text_input = ' '.join(text_input)\n    text_input = tokenizer.texts_to_sequences([text_input])\n    text_input = pad_sequences(text_input, maxlen=maxlen, padding='pre')\n    text_input = np.argmax(model.predict(text_input))\n    \n    if text_input == 0:\n        print('Predicted Emotion: Sadness')\n    elif text_input == 1:\n        print('Predicted Emotion: Joy')\n    elif text_input == 2:\n        print('Predicted Emotion: Love')\n    elif text_input == 3:\n        print('Predicted Emotion: Anger')\n    elif text_input == 4:\n        print('Predicted Emotion: Fear')\n    else:\n        print('Predicted Emotion: Surprise')\n    return text_input\n\nlabel_ = {0: \"Sadness\", 1: \"Joy\", 2: \"Love\", 3: \"Anger\", 4: \"Fear\", 5: \"Surprise\"}\ntest['label'] = test['label'].replace(label_)\n\n# Randomly chosen Test Dataset data points\ni = random.randint(0, len(test) - 1)\n\nprint('Test Text:', test['text'][i])\nprint(' ')\nprint('Actual Emotion:', test['label'][i])\nmake_predictions(test['text'][i])\nprint('-'*50)\nprint('Test Text:', test['text'][i+1])\nprint(' ')\nprint('Actual Emotion:', test['label'][i+1])\nmake_predictions(test['text'][i+1])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-13T05:31:03.601612Z","iopub.execute_input":"2021-09-13T05:31:03.602297Z","iopub.status.idle":"2021-09-13T05:31:03.786972Z","shell.execute_reply.started":"2021-09-13T05:31:03.602225Z","shell.execute_reply":"2021-09-13T05:31:03.785688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Confusion Matrix of the Test Data","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\nlabel_ = {\"Sadness\": 0, \"Joy\": 1, \"Love\": 2, \"Anger\": 3, \"Fear\": 4, \"Surprise\": 5}\ntest['label'] = test['label'].replace(label_)\n\npred = model.predict_classes(Padded_test)\nplt.figure(figsize=(15, 8))\nconf_mat = confusion_matrix(test['label'].values, pred)\nconf_mat = pd.DataFrame(conf_mat, columns=np.unique(test['label']), index=np.unique(pred))\nconf_mat.index.name = 'Actual'\nconf_mat.columns.name = 'Predicted'\nsns.heatmap(conf_mat, annot=True, fmt='g')\nplt.title('Confusion Matrix of the Test Data', fontsize=14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T05:31:03.78867Z","iopub.execute_input":"2021-09-13T05:31:03.789185Z","iopub.status.idle":"2021-09-13T05:31:04.417031Z","shell.execute_reply.started":"2021-09-13T05:31:03.789148Z","shell.execute_reply":"2021-09-13T05:31:04.415937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Having Fun with the Model","metadata":{}},{"cell_type":"code","source":"make_predictions('No one told you when to run, you missed the starting gun')","metadata":{"execution":{"iopub.status.busy":"2021-09-13T05:31:04.418798Z","iopub.execute_input":"2021-09-13T05:31:04.419236Z","iopub.status.idle":"2021-09-13T05:31:04.474431Z","shell.execute_reply.started":"2021-09-13T05:31:04.419184Z","shell.execute_reply":"2021-09-13T05:31:04.473434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_predictions(\"I just asked one question to confirm his request, and my boss bit my head off.\")","metadata":{"execution":{"iopub.status.busy":"2021-09-13T05:31:04.476057Z","iopub.execute_input":"2021-09-13T05:31:04.476477Z","iopub.status.idle":"2021-09-13T05:31:04.537896Z","shell.execute_reply.started":"2021-09-13T05:31:04.476437Z","shell.execute_reply":"2021-09-13T05:31:04.536555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_predictions(\"She’s flying high after the successful product launch.\")","metadata":{"execution":{"iopub.status.busy":"2021-09-13T05:31:04.539451Z","iopub.execute_input":"2021-09-13T05:31:04.539895Z","iopub.status.idle":"2021-09-13T05:31:04.591705Z","shell.execute_reply.started":"2021-09-13T05:31:04.539866Z","shell.execute_reply":"2021-09-13T05:31:04.590645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_predictions(\"I’m going to have the first meeting with a big client tomorrow, and I’m feeling butterflies in my stomach\")","metadata":{"execution":{"iopub.status.busy":"2021-09-13T05:31:04.59489Z","iopub.execute_input":"2021-09-13T05:31:04.595199Z","iopub.status.idle":"2021-09-13T05:31:04.646468Z","shell.execute_reply.started":"2021-09-13T05:31:04.595172Z","shell.execute_reply":"2021-09-13T05:31:04.645295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_predictions(\"Sometimes the people who appear to be the most confident are actually afraid of their own shadows.\")","metadata":{"execution":{"iopub.status.busy":"2021-09-13T05:31:04.648028Z","iopub.execute_input":"2021-09-13T05:31:04.648413Z","iopub.status.idle":"2021-09-13T05:31:04.699654Z","shell.execute_reply.started":"2021-09-13T05:31:04.648375Z","shell.execute_reply":"2021-09-13T05:31:04.698677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_predictions(\"I'm really impressed that Ashley can speak 7 languages, whereas I only speak one!\")","metadata":{"execution":{"iopub.status.busy":"2021-09-13T05:31:04.701262Z","iopub.execute_input":"2021-09-13T05:31:04.7018Z","iopub.status.idle":"2021-09-13T05:31:04.75276Z","shell.execute_reply.started":"2021-09-13T05:31:04.701716Z","shell.execute_reply":"2021-09-13T05:31:04.75163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_predictions(\"Grandpa was very proud of me when I got a promotion at work. He took me out to dinner to celebrate.\")","metadata":{"execution":{"iopub.status.busy":"2021-09-13T05:31:04.755959Z","iopub.execute_input":"2021-09-13T05:31:04.756241Z","iopub.status.idle":"2021-09-13T05:31:04.807491Z","shell.execute_reply.started":"2021-09-13T05:31:04.756214Z","shell.execute_reply":"2021-09-13T05:31:04.806307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_predictions(\"We are delighted that you will be coming to visit us. It will be so nice to have you here.\")","metadata":{"execution":{"iopub.status.busy":"2021-09-13T05:31:04.810885Z","iopub.execute_input":"2021-09-13T05:31:04.811182Z","iopub.status.idle":"2021-09-13T05:31:04.859971Z","shell.execute_reply.started":"2021-09-13T05:31:04.81114Z","shell.execute_reply":"2021-09-13T05:31:04.858899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_predictions(\"I am anxious to hear back about the job interview I had on Friday. I hope I get the job!\")","metadata":{"execution":{"iopub.status.busy":"2021-09-13T05:31:04.861415Z","iopub.execute_input":"2021-09-13T05:31:04.861872Z","iopub.status.idle":"2021-09-13T05:31:04.918173Z","shell.execute_reply.started":"2021-09-13T05:31:04.861801Z","shell.execute_reply":"2021-09-13T05:31:04.916968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References\n\nhttps://www.coursera.org/learn/tweet-emotion-tensorflow\n\nhttps://www.geeksforgeeks.org/python-lemmatization-with-nltk/","metadata":{}}]}